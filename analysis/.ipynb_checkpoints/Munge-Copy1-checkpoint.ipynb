{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas import DataFrame, Series\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data from our JSON file. \n",
    "The data is stored as a dictionary of dictionaries in the json file. We store it that way beacause it's easy to add data to the existing master data file. Also, I haven't figured out how to get it in a database yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with open('../pipeline/data/Day90ApartmentData.json') as f:\n",
    "    my_dict = json.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def listing_cleaner(entry):\n",
    "    print entry\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{u'content': 259, u'laundry': u'laundry in bldg', u'price': 675, u'time': u'00:44:11', u'dog': 0, u'bed': 0, u'bath': nan, u'feet': 300, u'date': u'2016-02-26', u'long': -122.797279, u'parking': u'street parking', u'lat': 45.860344, u'smoking': nan, u'getphotos': 8, u'cat': 0, u'hasmap': 1, u'wheelchair': nan, u'housingtype': u'apartment'}\n"
     ]
    }
   ],
   "source": [
    "listing_cleaner(my_dict['5465197037'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "type(dframe['bath']['5399866740'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean up the data a bit\n",
    "Right now the 'shared' and 'split' are included in number of bathrooms. If I were to convert that to a number I would consider a shared/split bathroom to be half or 0.5 of a bathroom."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dframe.bath = dframe.bath.replace('shared',0.5)\n",
    "dframe.bath = dframe.bath.replace('split',0.5)\n",
    "dframe.smoking = dframe.smoking.replace(np.nan, 'smoking')\n",
    "dframe.furnished = dframe.furnished.replace(np.nan,'not furnished')\n",
    "dframe.wheelchair = dframe.wheelchair.replace(np.nan, 'not wheelchair accessible')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dframe.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dframe.bed.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import Imputer, LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def meanimputer(column):\n",
    "    imp = Imputer(missing_values='NaN', strategy='mean', axis=1)\n",
    "    imp.fit(column)\n",
    "    X = imp.transform(column)\n",
    "    return X[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "arr = np.array([np.nan, 'house', 'boat', 'houseboat', 'house', np.nan, 'house','houseboat'])\n",
    "prac_df = DataFrame()\n",
    "prac_df['arr'] = arr\n",
    "prac_df['arr']\n",
    "modeimputer(prac_df['arr'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "def modeimputer(column):\n",
    "\n",
    "    le = LabelEncoder()\n",
    "    column = le.fit_transform(column)\n",
    "    print le.classes_\n",
    "    print type(le.classes_[0])\n",
    "    print column\n",
    "    nan = le.transform([np.nan])[0]\n",
    "    print nan\n",
    "    print type(column)\n",
    "    column = list(column)\n",
    "    for _,i in enumerate(column):\n",
    "        if i == nan:\n",
    "            column[_] = np.nan\n",
    "    \n",
    "    imp = Imputer(missing_values='NaN', strategy='most_frequent', axis=1)\n",
    "    imp.fit(column)\n",
    "\n",
    "    X = imp.transform(column)\n",
    "    \n",
    "    for _,i in enumerate(X[0]):\n",
    "        if np.isnan(i):\n",
    "            X[_] = 0\n",
    "    X = X.astype(int)\n",
    "\n",
    "\n",
    "    Y = le.inverse_transform(X)\n",
    "\n",
    "    return Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.base import TransformerMixin\n",
    "class ModeImputer(TransformerMixin):\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"Impute missing values.\n",
    "\n",
    "        Columns of dtype object are imputed with the most frequent value \n",
    "        in column.\n",
    "\n",
    "        Columns of other types are imputed with mean of column.\n",
    "        \n",
    "        Credit:http://stackoverflow.com/questions/25239958/\n",
    "        impute-categorical-missing-values-in-scikit-learn\n",
    "\n",
    "        \"\"\"\n",
    "    def fit(self, X, y=None):\n",
    "\n",
    "        self.fill = pd.Series([X[c].value_counts().index[0]\n",
    "            if X[c].dtype == np.dtype('O') else X[c].mean() for c in X],\n",
    "            index=X.columns)\n",
    "\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        return X.fillna(self.fill)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = [\n",
    "    ['a', 1, 2],\n",
    "    ['b', 1, 1],\n",
    "    ['b', 2, 2],\n",
    "    [np.nan, np.nan, np.nan]\n",
    "]\n",
    "\n",
    "X = pd.DataFrame(data)\n",
    "xt = ModeImputer().fit_transform(X)\n",
    "\n",
    "print('before...')\n",
    "print(X)\n",
    "print('after...')\n",
    "print(xt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dframe = ModeImputer().fit_transform(dframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dframe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dframe.describe(include = 'all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dframe.bed.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dframe.parking.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "u_dframe = DataFrame()\n",
    "dframe['bath'] = meanimputer(dframe['bath'])\n",
    "dframe['bed'] = meanimputer(dframe['bed'])\n",
    "dframe['feet'] = meanimputer(dframe['feet'])\n",
    "dframe['lat'] = meanimputer(dframe['lat'])\n",
    "dframe['long'] = meanimputer(dframe['long'])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dframe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dframe.describe(include='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = dframe[dframe.lat > 45.4][dframe.lat < 45.6][dframe.long < -122.0][dframe.long > -123.5]\n",
    "plt.figure(figsize=(15,10))\n",
    "plt.scatter(data = data, x = 'long',y='lat')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### It looks like Portland!!!\n",
    "Let's cluster the data. Start by creating a list of [['lat','long'], ...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "XYdf = dframe[dframe.lat > 45.4][dframe.lat < 45.6][dframe.long < -122.0][dframe.long > -123.5]\n",
    "data = [[XYdf['lat'][i],XYdf['long'][i]] for i in XYdf.index]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use K Means Clustering because that's the clustering method I recently learned in class! There may be others that work better, but this is the tool that I know"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "km = KMeans(n_clusters=40)\n",
    "km.fit(data)\n",
    "neighborhoods = km.cluster_centers_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%pylab inline\n",
    "figure(1,figsize=(20,12))\n",
    "plot([row[1] for row in data],[row[0] for row in data],'b.')\n",
    "for i in km.cluster_centers_:  \n",
    "    plot(i[1],i[0], 'g*',ms=25)\n",
    "'''Note to Riley: come back and make it look pretty'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We chose our neighborhoods!\n",
    "I've found that every once in a while the centers end up in different points, but are fairly consistant. Now let's process our data points and figure out where the closest neighborhood center is to it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "neighborhoods = neighborhoods.tolist()\n",
    "for i in enumerate(neighborhoods):\n",
    "    i[1].append(i[0])\n",
    "print neighborhoods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a function that will label each point with a number coresponding to it's neighborhood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def clusterer(X, Y,neighborhoods):\n",
    "    neighbors = []\n",
    "    for i in neighborhoods:\n",
    "        distance = ((i[0]-X)**2 + (i[1]-Y)**2)\n",
    "        neighbors.append(distance)\n",
    "    closest = min(neighbors)\n",
    "    return neighbors.index(closest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "neighborhoodlist = []\n",
    "for i in dframe.index:\n",
    "    neighborhoodlist.append(clusterer(dframe['lat'][i],dframe['long'][i],neighborhoods))\n",
    "dframe['neighborhood'] = neighborhoodlist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Here's the new Part. We're breaking out the neighborhood values into their own columns. Now the algorithms can read them as categorical data rather than continuous data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "def CategoricalToBinary(dframe,column_name):\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    listy = le.fit_transform(dframe[column_name])\n",
    "    dframe[column_name] = listy\n",
    "    unique = dframe[column_name].unique()\n",
    "    serieslist = [list() for _ in xrange(len(unique))]\n",
    "    \n",
    "    \n",
    "    for column, _ in enumerate(serieslist):\n",
    "        for i, item in enumerate(dframe[column_name]):\n",
    "            if item == column:\n",
    "                serieslist[column].append(1)\n",
    "            else:\n",
    "                serieslist[column].append(0)\n",
    "        dframe[column_name+str(column)] = serieslist[column]\n",
    "\n",
    "   \n",
    "    return dframe\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pd.set_option('max_columns', 100)\n",
    "dframe = CategoricalToBinary(dframe,'housingtype')\n",
    "dframe = CategoricalToBinary(dframe,'parking')\n",
    "dframe = CategoricalToBinary(dframe,'laundry')\n",
    "dframe = CategoricalToBinary(dframe,'smoking')\n",
    "dframe = CategoricalToBinary(dframe,'wheelchair')\n",
    "dframe = CategoricalToBinary(dframe,'neighborhood')\n",
    "dframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dframe = dframe.drop('date',1)\n",
    "dframe = dframe.drop('housingtype',1)\n",
    "dframe = dframe.drop('parking',1)\n",
    "dframe = dframe.drop('laundry',1)\n",
    "dframe = dframe.drop('smoking',1)\n",
    "dframe = dframe.drop('wheelchair',1)\n",
    "dframe = dframe.drop('neighborhood',1)\n",
    "dframe = dframe.drop('time',1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "columns=list(dframe.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "print len(dframe)\n",
    "df2 = dframe[dframe.price < 10000][columns].dropna()\n",
    "print len(df2)\n",
    "print len(df2)/len(dframe)\n",
    "\n",
    "price = df2[['price']].values\n",
    "columns.pop(columns.index('price'))\n",
    "features = df2[columns].values\n",
    "\n",
    "from sklearn.cross_validation import train_test_split\n",
    "features_train, features_test, price_train, price_test = train_test_split(features, price, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, lets put it through Decision Tree!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What about Random Forest?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "reg = RandomForestRegressor()\n",
    "reg = reg.fit(features_train, price_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "forest_pred = reg.predict(features_test)\n",
    "forest_pred = np.array([[item] for item in forest_pred])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print r2_score(forest_pred, price_test)\n",
    "plt.scatter(forest_pred,price_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df2['predictions'] = reg.predict(df2[columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df2['predictions_diff'] = df2['predictions']-df2['price']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sd = np.std(df2['predictions_diff'])\n",
    "sns.kdeplot(df2['predictions_diff'][df2['predictions_diff']>-150][df2['predictions_diff']<150])\n",
    "sns.plt.xlim(-150,150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = df2[dframe.lat > 45.45][df2.lat < 45.6][df2.long < -122.4][df2.long > -122.8][df2['predictions_diff']>-150][df2['predictions_diff']<150]\n",
    "plt.figure(figsize=(15,10))\n",
    "plt.scatter(data = data, x = 'long',y='lat', c = 'predictions_diff',s=10,cmap='coolwarm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print np.mean([1,2,34,np.nan])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def averager(dframe):\n",
    "    dframe = dframe.T\n",
    "    dframe.dropna()\n",
    "    averages = {}\n",
    "    for listing in dframe:\n",
    "        try:\n",
    "            key = str(dframe[listing]['bed'])+','+str(dframe[listing]['bath'])+','+str(dframe[listing]['neighborhood'])+','+str(dframe[listing]['feet']-dframe[listing]['feet']%50)\n",
    "            if key not in averages:\n",
    "                averages[key] = {'average_list':[dframe[listing]['price']], 'average':0}\n",
    "            elif key in averages:\n",
    "                averages[key]['average_list'].append(dframe[listing]['price'])\n",
    "        except TypeError:\n",
    "            continue\n",
    "    for entry in averages:\n",
    "        averages[entry]['average'] = np.mean(averages[entry]['average_list'])\n",
    "    return averages\n",
    "            \n",
    "        \n",
    "            \n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "averages = averager(dframe)\n",
    "print averages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dframe['averages']= averages[str(dframe['bed'])+','+str(dframe['bath'])+','+str(dframe['neighborhood'])+','+str(dframe['feet']-dframe['feet']%50)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dframe.T\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow! up to .87! That's our best yet! What if we add more trees???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "reg = RandomForestRegressor(n_estimators = 100)\n",
    "reg = reg.fit(features_train, price_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "forest_pred = reg.predict(features_test)\n",
    "forest_pred = np.array([[item] for item in forest_pred])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print r2_score(forest_pred, price_test)\n",
    "print plt.scatter(pred,price_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "reg = DecisionTreeRegressor(max_depth = 5)\n",
    "reg.fit(features_train, price_train)\n",
    "print len(features_train[0])\n",
    "columns = [str(x) for x in columns]\n",
    "print columns\n",
    "from sklearn.tree import export_graphviz\n",
    "export_graphviz(reg,feature_names=columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Up to .88!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So what is our goal now? I'd like to see if adjusting the number of neighborhoods increases the accuracy. same for the affect with the number of trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def neighborhood_optimizer(dframe,neighborhood_number_range, counter_num):\n",
    "    XYdf = dframe[dframe.lat > 45.4][dframe.lat < 45.6][dframe.long < -122.0][dframe.long > -123.5]\n",
    "    data = [[XYdf['lat'][i],XYdf['long'][i]] for i in XYdf.index]\n",
    "    r2_dict = []\n",
    "    for i in neighborhood_number_range:\n",
    "        counter = counter_num\n",
    "        average_accuracy_list = []\n",
    "        while counter > 0:\n",
    "            km = KMeans(n_clusters=i)\n",
    "            km.fit(data)\n",
    "            neighborhoods = km.cluster_centers_\n",
    "            neighborhoods = neighborhoods.tolist()\n",
    "            for x in enumerate(neighborhoods):\n",
    "                x[1].append(x[0])\n",
    "            neighborhoodlist = []\n",
    "            for z in dframe.index:\n",
    "                neighborhoodlist.append(clusterer(dframe['lat'][z],dframe['long'][z],neighborhoods))\n",
    "            dframecopy = dframe.copy()\n",
    "            dframecopy['neighborhood'] = Series((neighborhoodlist), index=dframe.index)\n",
    "            df2 = dframecopy[dframe.price < 10000][['bath','bed','feet','dog','cat','content','getphotos', 'hasmap', 'price','neighborhood']].dropna()\n",
    "            features = df2[['bath','bed','feet','dog','cat','content','getphotos', 'hasmap', 'neighborhood']].values\n",
    "            price = df2[['price']].values\n",
    "            features_train, features_test, price_train, price_test = train_test_split(features, price, test_size=0.1)\n",
    "            reg = RandomForestRegressor()\n",
    "            reg = reg.fit(features_train, price_train)\n",
    "            forest_pred = reg.predict(features_test)\n",
    "            forest_pred = np.array([[item] for item in forest_pred])\n",
    "            counter -= 1\n",
    "            average_accuracy_list.append(r2_score(forest_pred, price_test))\n",
    "        total = 0\n",
    "        for entry in average_accuracy_list:\n",
    "            total += entry\n",
    "        r2_accuracy = total/len(average_accuracy_list)\n",
    "        r2_dict.append((i,r2_accuracy))\n",
    "    print r2_dict\n",
    "    return r2_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "neighborhood_number_range = [i for _,i in enumerate(range(2,31,2))]\n",
    "neighborhood_number_range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "r2_dict = neighborhood_optimizer(dframe,neighborhood_number_range,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "r2_dict[:][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.scatter([x[0] for x in r2_dict],[x[1] for x in r2_dict])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like the optimum is right around 10 or 11, and then starts to drop off. Let's get a little more granular and look at a smaller range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "neighborhood_number_range = [i for _,i in enumerate(range(7,15))]\n",
    "neighborhood_number_range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "r2_dict = neighborhood_optimizer(dframe,neighborhood_number_range,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print r2_dict\n",
    "plt.scatter([x[0] for x in r2_dict],[x[1] for x in r2_dict])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trying a few times, it looks like 10, 11 and 12 get the best results at ~.85. Of course, we'll need to redo some of these optomizations after we properly process our data. Hopefully we'll see some more consistency then too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "r2_dict = neighborhood_optimizer(dframe,[10,11,12],25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note #1 to Riley: (From Last time) Perhaps look into another regressor? see if there's one that's inherantly better at this kind of thing.\n",
    "\n",
    "Note #2 to Riley: Figure out how to process data so that you don't have to drop null values\n",
    "\n",
    "Note #3 to Riley: convert categorical data into binary\n",
    "\n",
    "Note #4 to Riley: I wonder if increasing the number of neighborhoods would become more accurate as we collect more data? like you could create a bunch of little accurate models instead of a bunch of bigger ones.\n",
    "\n",
    "Learned: If you plan on using Decision Tree/Random Forest from SKLearn, make sure you collect your discrete variables in separate columns and make them binary yes or no(0 or 1)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
